{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import linear_model\nimport xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\nimport catboost\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom IPython.display import HTML\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# load train data and split them into nemric and catgorial Data Frame\ndf = pd.read_csv('../input/train.csv', index_col='Id')\nX_df = df.drop('SalePrice', axis=1)\ny_df = df.SalePrice\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric_features = X_df.select_dtypes(include=numerics)\ncat_features = X_df.select_dtypes(exclude=numerics)\nX_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# three pre processor methods\n\nQUANTILE_THRESHOLD = 0.3\n\ndef numeric_features_processor(X):\n    \"\"\"\n    the methods deletes features with too many null obs\n    and features with too low variance\n    \"\"\"\n    total = X.isnull().sum().sort_values(ascending=False)\n    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    name_features_to_delete = missing_data.head(1).index\n    numeric_features = X.drop(name_features_to_delete, axis=1)\n    # var threshold\n    threshold_var = numeric_features.var().quantile([QUANTILE_THRESHOLD]).values[0]\n    var_df = pd.DataFrame(numeric_features.var())\n    var_cols_names = var_df[var_df[0]>threshold_var].index\n    return numeric_features[var_cols_names]\n\ndef cat_features_processor(X, test):\n    \"\"\"\n    the methods deletes features with too many null obs, features with too low variance\n    and re-maps all categiral features that supose to be made with label-encoding.\n    then it returns data-frame after calling to pandas methos - get_dummies\n    return: Pandas Data Frame\n    \"\"\"\n    #delete features with too many missing data\n    if not test:\n        total = X.isnull().sum().sort_values(ascending=False)\n        percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n        missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        features_names_selected =  missing_data.loc[missing_data['Percent'] < 0.15].index\n        cat_features = X[features_names_selected]\n    else:\n        cat_features = X\n    col_names = []\n    for col in cat_features:\n        if 'Gd' in cat_features[col].unique():\n            col_names.append(col)\n            cat_features[col] = cat_features[col].map({np.nan:0,'No':1, 'Mn':2, 'Av':3,\n                                               'Po':1, 'Fa':2, 'TA':3,'Gd':4 ,'Ex':5 })\n    # make all the rest as dumies\n    return pd.get_dummies(cat_features)\n\ndef features_pre_processing(X, test=False):\n    \"\"\"\n    splits the Data_Frame into two seperate DataFrames, one with numeric and other with\n    categorial Features DataFrame then sends them to the relevate methods (numeric_features_processor &\n    cat_features_processor).\n    sort every data frame by cols names.\n    return: a tuple of two Data Frames\n    \"\"\"\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric_features = X.select_dtypes(include=numerics)\n    cat_features = X.select_dtypes(exclude=numerics)\n    \n    df_num = numeric_features_processor(numeric_features)\n    df_num = df_num.reindex(sorted(df_num.columns), axis=1) #order by col's names\n    df_cat = cat_features_processor(cat_features, test)\n    df_cat = df_cat.reindex(sorted(df_cat.columns), axis=1) #order by col's names\n    return df_num, df_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Feature_selector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    a Transformer that knows how to select the best features of every dataset\n    using combination of SelectKBest and RandomForrest\n    \n    inheritage:\n    -BaseEstimator:\n    implements get_params() and set_params()\n    and \n    -TransformerMixin:\n    implements automatic fit_transform method\n    \"\"\"\n    def __init__(self, category=False):\n        self.category = category\n        \n    def fit(self,X, y=None):\n        \"\"\"\n        gets the X data set and checks the best Features based on the y data set\n        and saves the Features names in a class member\n        \"\"\"\n        if self.category:\n            self.X_coloums_names = \\\n            categiral_transformer.steps[0][1].get_feature_names(train_cat_features.columns)\n        else:\n            self.X_coloums_names = \\\n            numeric_transformer.steps[1][1].get_feature_names(train_num_features.columns)\n        \n        regr = RandomForestRegressor(max_depth=4, random_state=42,n_estimators=100)\n        regr.fit(X, y_df)\n        feature_importances_by_rand_forrest = pd.Series(regr.feature_importances_, \n                        index=self.X_coloums_names).sort_values(ascending=False)\n        \n        kbest_selector = SelectKBest(f_regression, k=20)\n        kbest_selector.fit_transform(X, y_df)\n        feature_importances_by_kbest = pd.Series(kbest_selector.scores_,\n                                         index=self.X_coloums_names).sort_values(ascending=False)\n        \n        df_feature_importances = pd.DataFrame([feature_importances_by_rand_forrest,\n                                               feature_importances_by_kbest],\n                                     index=['rand_forest', 'kbest']).transpose()\n        \n        self.selected_features_names = df_feature_importances.loc[\n                           (df_feature_importances['rand_forest'] > 0) | \n                           (df_feature_importances['kbest'] > \n                            df_feature_importances['kbest'].quantile([0.95]).values[0])].index\n        \n        return self\n    \n    def transform(self, X, y=None):\n        \"\"\"\n        return the best featres of the data set that was fitted by\n        \"\"\"\n        X = pd.DataFrame(X, columns=self.X_coloums_names)\n        selected_features = X[self.selected_features_names]\n        return selected_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make piplines:\n\nnumeric_transformer = Pipeline([('imput',SimpleImputer(strategy='median')),\n                                ('extract',PolynomialFeatures(2 ,include_bias=False)),\n                                ('select', Feature_selector())])\ncategiral_transformer = Pipeline([('extract',PolynomialFeatures(2 ,include_bias=False)),\n                                  ('select', Feature_selector(category=True))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creates the features dfs:\ntrain_num_features, train_cat_features = features_pre_processing(X_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# runs dataframes trought the piplines\ntrain_num_features = numeric_transformer.fit_transform(train_num_features)\ntrain_num_features_names = train_num_features.columns\n\ntrain_cat_features = categiral_transformer.fit_transform(train_cat_features)\ntrain_cat_features_names = train_cat_features.columns\n\n# concat the data frames into one\ndf_X_post_process = pd.concat([train_num_features, train_cat_features], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train, X_test, y_train, y_test = train_test_split(df_X_post_process, y_df, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model by XGBOOST\ndf_X_post_process = df_X_post_process.reindex(sorted(df_X_post_process.columns), axis=1) # sort by cols names\n\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 104)\nxgb_reg.fit(df_X_post_process, y_df)\nprint(xgb_reg.score(df_X_post_process, y_df))#, xg_reg.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv', index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_num_features, test_cat_features = features_pre_processing(test_df, test=True)\ntest_num_cols, test_cat_cols = test_num_features.columns, test_cat_features.columns\ntest_num_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# process numeric features\nnumeric_test_transformer = Pipeline([('imput',SimpleImputer(strategy='median')),\n                                ('extract',PolynomialFeatures(2 ,include_bias=False))])\ntest_num_features = numeric_test_transformer.fit_transform(test_num_features)\ntest_num_features = pd.DataFrame(test_num_features,\n                                 columns=numeric_test_transformer.steps[1][1].get_feature_names(test_num_cols))\ntest_num_features = test_num_features[train_num_features_names]\ntest_num_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# process cat features\ncat_test_extract = PolynomialFeatures(2 ,include_bias=False)\ntest_cat_features = cat_test_extract.fit_transform(test_cat_features)\n\ntest_cat_features = pd.DataFrame(test_cat_features,\n                                 columns=cat_test_extract.get_feature_names(test_cat_cols))\n# select importance features\n## this way of selecting ignores un exists cols in test data - it may happens because not all of the\n## values in every featres are actualy exists so make dummies wont make the same features as in the\n## train data set\ntest_cat_features = test_cat_features.loc[:,test_cat_features.columns.isin(train_cat_features_names)]\ntest_cat_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_X_test_post_process = pd.concat([test_num_features, test_cat_features], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### create the un exists features in the test data set which exists in the train data sets (all of them dummies)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df_X_post_process:\n    if col not in df_X_test_post_process.columns:\n        df_X_test_post_process[col] = 0\n \n# sort by cols names\ndf_X_test_post_process = df_X_test_post_process.reindex(sorted(df_X_test_post_process.columns), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Download method of the submission part"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(xgb_reg.predict(df_X_test_post_process), columns=['SalePrice'], index=test_df.index)\n\ndf.to_csv('submission.csv')\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}